<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="">

  
  <link rel="alternate" hreflang="en-us" href="https://paulobreviglieri.com/detecting-pneumonia/">

  


  
  
  
  <meta name="theme-color" content="#000080">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha256-FMvZuGapsJLjouA6k7Eo2lusoAX9i0ShlWFG6qt7SLc=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=UA-165357798-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url, target) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           if (target !== '_blank') {
             document.location = url;
           }
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target, event.target.getAttribute('target'));  
  }

  gtag('js', new Date());
  gtag('config', 'UA-165357798-1', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hue0b5dbdcd29e15f3bb2c3e480ec18a0b_14701_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hue0b5dbdcd29e15f3bb2c3e480ec18a0b_14701_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://paulobreviglieri.com/detecting-pneumonia/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="twitter:site" content="@pcbreviglieri">
  <meta property="twitter:creator" content="@pcbreviglieri">
  
  <meta property="og:site_name" content="Paulo Breviglieri | Data Science &amp; Machine Learning">
  <meta property="og:url" content="https://paulobreviglieri.com/detecting-pneumonia/">
  <meta property="og:title" content="Detecting Pneumonia with Convolutional Neural Networks | Paulo Breviglieri | Data Science &amp; Machine Learning">
  <meta property="og:description" content=""><meta property="og:image" content="https://paulobreviglieri.com/images/logo.svg">
  <meta property="twitter:image" content="https://paulobreviglieri.com/images/logo.svg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-01-01T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2020-01-01T00:00:00&#43;00:00">
  

  



  


  


  





  <title>Detecting Pneumonia with Convolutional Neural Networks | Paulo Breviglieri | Data Science &amp; Machine Learning</title>

</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class=" ">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  











  


<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/"><img src="/images/logo.svg" alt="Paulo Breviglieri | Data Science &amp; Machine Learning"></a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/"><img src="/images/logo.svg" alt="Paulo Breviglieri | Data Science &amp; Machine Learning"></a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/portfolio"><span>Portfolio</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/about"><span>About</span></a>
        </li>

        
        

        

        
        
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/" data-target="[]"><span></span></a>
        </li>

        
        

      

        

        
        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link" href="https://linkedin.com/in/pcbreviglieri" target="_blank" rel="noopener"><span><i class="icon-new-icon-linkedin-color" style="font-size: 1rem; line-height: 1"></i></span></a>
        </li>

        

        
        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link" href="https://kaggle.com/pcbreviglieri" target="_blank" rel="noopener"><span><i class="icon-new-icon-kaggle-color" style="font-size: 1rem; line-height: 1"></i></span></a>
        </li>

        

        
        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link" href="https://github.com/pcbreviglieri" target="_blank" rel="noopener"><span><i class="icon-new-icon-github-color" style="font-size: 1rem; line-height: 1"></i></span></a>
        </li>

        

        
        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link" href="https://gitlab.com/pcbreviglieri" target="_blank" rel="noopener"><span><i class="icon-new-icon-gitlab-color" style="font-size: 1rem; line-height: 1"></i></span></a>
        </li>

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>



  
<span class="js-widget-page d-none"></span>





  
  
  
  




  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-foreword" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1 style="text-align: center; font-size: 30px; color: #ff3000;">Detecting Pneumonia with Convolutional Neural Networks</h1>
<p>May 20, 2020 - By Paulo Breviglieri</p>
<h1>Foreword</h1>
<p>The 2020 COVID-19 outbreak has prompted for increased research on several aspects of the disease. The <b>diagnosis of pneumonia</b> (one of the primary complications resulting from COVID-19) through imaging techniques and the particular use of artificial intelligence as an ancilary tool has gained special traction as the disease progresses.</p>
<p>This study is inspired by the current gravity of the COVID-19 pandemic. It is though not intended to serve as a medical reference in any sense. Instead, the main purpose of this exercise is to revisit traditional machine learning-based image processing and explore the level of confidence obtained in chest x-ray analysis as a pneumonia diagnosis instrument.</p>
<p><b>Warning</b>: computer vision machine learning exercises are time and resource consuming. This work was indeed developed in GPU-based runtime environments provided by Kaggle and Google Colab for code execution. Unless you have a very powerful machine to execute your code on a local host, consider a similar cloud-based approach.</p>
<p>Logic enhancement and code forking are welcome and encouraged provided that proper referencing to this work is made. Latest code version may be found in the author's <a href="https://github.com/pcbreviglieri" target="_blank">GitHub</a> and <a href="https://gitlab.com/pcbreviglieri" target="_blank">GitLab</a> repositories. Thank you.</p>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-01-introduction" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>1. Introduction</h1>
<h2>1.1. The COVID-19 pandemic</h2>
<p>Corona Virus Disease 2019 (COVID-19) severely affected many countries globally over the  first half of 2020. COVID-19 is an infectious disease caused by the most recently discovered coronavirus, formally denominated 'Severe Acute Respiratory Syndrome Coronavirus 2' (SARS-CoV-2), unknown before the outbreak observed in Wuhan, China.</p>
<p>COVID-19 is contagious in humans. SARS-CoV-2 primarily spreads among individuals through physical contact and via respiratory droplets originated from coughing or sneezing. The virus seizes human cells by binding to the angiotensin receptor converting enzyme 2 (ACE2).</p>
<p>In humans, corona viruses are known to cause respiratory infections ranging from a common cold to more severe diseases such as the Middle East Respiratory Syndrome (MERS) and Severe Acute Respiratory Syndrome (SARS). In the case of COVID-19, usual symptoms include fever, cough, fatigue, shortness of breath, smell and taste disturbs.</p>
<div class='row'>
    <img class="imageboxcentered" src='/detecting-pneumonia/images/SARS-CoV-2.png' alt='SARS-CoV-2'>
</div>
<p style="text-align: center">Illustration of a SARS-CoV-2 virion <a href="https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2" target="_blank">(<em>Wikipedia</em>)</a></p>
<p>SARS-CoV-2 may spread through respiratory airways reaching the lungs, where it may lead to the development of severe viral pneumonia, identified as the final death cause in most lethal cases of COVID-19. Lung monitoring for pneumonia has gained greater importance over the COVID-19 outbreak.</p>
<h2>1.2. Pneumonia</h2>
<p>Per <a href="https://en.wikipedia.org/wiki/Pneumonia" target="_blank">Wikipedia</a>,</p>
<p style="text-align: center; padding-left: 5em; padding-right: 5em">"<em><b>Pneumonia</b> is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli. Symptoms typically include some combination of productive or dry cough, chest pain, fever and difficulty breathing. (...) Pneumonia is usually caused by infection with viruses or bacteria and less commonly by other microorganisms, certain medications or conditions such as autoimmune diseases. Risk factors include cystic fibrosis, chronic obstructive pulmonary disease (COPD), sickle cell disease, asthma, diabetes, heart failure, a history of smoking, a poor ability to cough such as following a stroke and a weak immune system. Diagnosis is often based on symptoms and physical examination. <b>Chest X-ray</b>, blood tests, and culture of the sputum may help confirm the diagnosis. (...) Each year, pneumonia affects about 450 million people globally (7% of the population) and results in about 4 million deaths.</em>"</p>
<p>Along with physical examination, <b>imaging diagnosis</b> plays a central role in the detection of pneumonia. <b>Chest radiographs</b> are frequently used in diagnosis procedures and represent a fast, cost-effective alternative to map the nature, features and extension of lung inflammations. X-ray radiograph image <b>opacity areas</b> are commonly correlated to pneumonia affected regions.</p>
<div class='row'>
    <img class="imageboxcentered" src='/detecting-pneumonia/images/pneumonia-normal-abnormal.png' alt='Normal and abnormal chest radiographs'>
</div>
<p style="text-align: center">Normal (left) and abnormal (right) chest radiographs - Increased opacity suggests a pneumonia occurrence</p>
<p>The identification of opacities in chest x-ray images is sometimes challenging, even for experienced practitioners. Artificial intelligence and computer vision techniques have been extensively used over the past few years as ancillary tools in pneumonia diagnosis.</p>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-02-objectives" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>2. Objectives of this deep learning exercise</h1>
<p>The primary goal of this exercise is educational. The role of computer vision  deep learning techniques - <b>Convolutional Neural Networks</b> (CNNs), in particular - is assessed in detail herein. Special attention is given to particular aspects of a CNN-based solution to chest radiograph opacity interpretation, including the choice and tuning of critical <b>hyperparameters</b> and the relevance of dataset <b>augmentation</b> procedures.</p>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-03-the-dataset" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>3. The dataset</h1>
<p>The dataset chosen for this deep learning exercise is a modified version of that submitted by Paul Mooney at <a href="https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia" target="_blank">Kaggle</a>, 'Chest X-Ray Images (Pneumonia)'. Per the dataset author, "<em>(...) Chest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children’s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients’ routine clinical care. (...)</em></p>
<p>The original dataset contains 5,863 observations, split into training (1,341 normal cases, 3,875 pneumonia cases), validation (8 normal cases, 8 pneumonia cases) and testing (234 normal cases, 390 pneumonia cases) folders.</p>
<p>The modified <a href="https://www.kaggle.com/pcbreviglieri/pneumonia-xray-images" target="_blank">dataset</a> contains the very same 5,863 observations, where a more balanced split between training  and validation images is proposed:</p>
<ul>
    <li>Training observations: 4,192 (1,082 normal cases, 3,110 lung opacity cases);</li>
    <li>Validation observations: 1,040 (267 normal cases, 773 lung opacity cases);</li>
    <li>Testing observations: 624 (234 normal cases, 390 lung opacity cases).</li>
</ul>
<p>This dataset is particularly interesting as:</p>
<ul>
    <li>it addresses a <b>specific segment</b> (<b>children</b>) critically susceptible to pneumonia;</li>
    <li>the number of observations, though not small, is also not large, allowing for the exploration of <b>augmentation</b> techniques;</li>
    <li>unlike other similar datasets, it contains images of several, non-standardized resolutions.</li>
</ul>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-04-initial-setup" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>4. Initial setup</h1>
<h2>4.1. Importing required libraries</h2>
<p>Along with traditional libraries imported for tensor manipulation, mathematical operations and graphics, one scikit-learn module (confusion_matrix for performance metric assessment) and specific <b>Keras image preprocessing</b> (<a href="https://keras.io/api/preprocessing/image/#imagedatagenerator-class" target="_blank">ImageDataGenerator</a>) and <b>deep learning</b> objects (<a href="https://keras.io/api/models/sequential/" target="_blank">Sequential</a>, <a href="https://keras.io/api/layers/convolution_layers/convolution2d/" target="_blank">Conv2D</a>, <a href="https://keras.io/api/layers/pooling_layers/max_pooling2d/" target="_blank">MaxPooling2D</a>, <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten" target="_blank">Flatten</a>, <a href="https://keras.io/api/layers/core_layers/dense/" target="_blank">Dense</a>) are used in this exercise.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> pandas <span style="color:#f92672">as</span> pd
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">from</span> PIL <span style="color:#f92672">import</span> Image

<span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix

<span style="color:#f92672">from</span> keras.preprocessing.image <span style="color:#f92672">import</span> ImageDataGenerator, array_to_img
<span style="color:#f92672">from</span> keras.models <span style="color:#f92672">import</span> Sequential
<span style="color:#f92672">from</span> keras.layers <span style="color:#f92672">import</span> Conv2D, MaxPooling2D, Flatten, Dense

<span style="color:#f92672">from</span> datetime <span style="color:#f92672">import</span> datetime

start_time <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()
</code></pre></div></div>
<p class="codeoutput">Using TensorFlow backend.</p>
<h2>4.2. Critical hyperparameters</h2>
<p>Machine learning hyperparameters are parameters whose value is set before the learning process starts. Several rounds of this exercise showed that specific hyperparameters play an important role when it comes to final prediction accuracy and execution runtime, reason why they have been segregated in the code session below:</p>
<ul>
    <li>'<b>hyper_dimension</b>': target image width and length in pixels considered when original images need to be rescaled for processing;</li>
    <li>'<b>hyper_epochs</b>': number of epochs (leaning iterations through which the whole dataset is exposed to the machine for weight updates);</li>
    <li>'<b>hyper_batch_size</b>': size of image batches;</li>
    <li>'<b>hyper_feature_maps</b>': reference number of feature maps generated by convolutional layers;</li>
    <li>'<b>hyper_channels</b>' and 'hyper_mode': number of channels utilized in the learning process. For colored RGB images, hyper_channels = 3 and hyper_mode = 'rgb', yet for grayscale images hyper_channels = 1 and hyper_mode = 'grayscale'.</li>
</ul>
<p>Specific considerations on hyperparameters are provided through the exercise.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">hyper_dimension <span style="color:#f92672">=</span> <span style="color:#ae81ff">500</span>
hyper_epochs <span style="color:#f92672">=</span> <span style="color:#ae81ff">100</span>
hyper_batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
hyper_feature_maps <span style="color:#f92672">=</span> <span style="color:#ae81ff">32</span>
hyper_channels <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
hyper_mode <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;grayscale&#39;</span>
</code></pre></div></div>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-05-deep-learning" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>5. Deep learning</h1>
<h2>5.1. Creating and compiling the deep learning model</h2>
<p>In this exercise a traditional convolutional neural network aimed at image processing and interpretation is adopted. The architecture, a Keras sequential model graphically depicted below, comprises:</p>
<ol>
    <li><b>Three pairs of convolution / pooling layers</b>: first and second produce 32 feature maps processed by subsequent 2x2 pooling engines generating 32 pooled maps, while the third produces 64 feature maps also processed by a subsequent 2x2 pooling engine generating a 64 pooled map. Convolution is performed with 3x3xk kernels, where k = 1 for grayscale images and k = 3 for RGB images. The input shape is a 500x500 pixels image (1 or 3 channels, as described above). 'ReLU' (Rectifier Linear Unit) is the activation function of choice;</li>
    <li>A <b>flattening layer</b>, responsible to provide the subsequent artificial neural network (ANN) with an flaattened, unidimensional tensor with the output of the last pooling layer;</li>
    <li>A <b>full connection pair of layers</b> with 64 and 1 neuron, respectively, responsible to perform binary classification ('normal' or 'pneumonia').</li>
</ol>
<div class='row'>
    <img class="imageboxcentered" src='/detecting-pneumonia/images/cnn.png' alt='convolutional neural network'>
</div>
<p>The model is compiled using '<a href="https://keras.io/api/optimizers/adam/" target="_blank">adam</a>' (stochastic gradient descent method based on adaptive estimation of first-order and second-order moments) as the <b>optimization function</b>. Since this is a classification problem ('normal' versus 'pneumonia' cases), '<a href="https://keras.io/api/losses/probabilistic_losses/#binary_crossentropy-function" target="_blank">binary_crossentropy</a>' is elected as the loss function. Performance will be assessed via the '<b>accuracy</b>' metric.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Initializing the CNN</span>
classifier <span style="color:#f92672">=</span> Sequential()

<span style="color:#75715e"># Convolution &amp; Pooling - First convolution layer</span>
classifier<span style="color:#f92672">.</span>add(Conv2D(hyper_feature_maps, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>),
                      input_shape <span style="color:#f92672">=</span> (hyper_dimension,
                                     hyper_dimension,
                                     hyper_channels),
                      activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>))
classifier<span style="color:#f92672">.</span>add(MaxPooling2D(pool_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)))

<span style="color:#75715e"># Convolution &amp; Pooling - Second convolution layer (same as first layer)</span>
classifier<span style="color:#f92672">.</span>add(Conv2D(hyper_feature_maps, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>),
                      input_shape <span style="color:#f92672">=</span> (hyper_dimension,
                                     hyper_dimension,
                                     hyper_channels),
                      activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>))
classifier<span style="color:#f92672">.</span>add(MaxPooling2D(pool_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)))

<span style="color:#75715e"># Convolution &amp; Pooling - Third convolution layer</span>
classifier<span style="color:#f92672">.</span>add(Conv2D(hyper_feature_maps <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">3</span>),
                      input_shape <span style="color:#f92672">=</span> (hyper_dimension,
                                     hyper_dimension,
                                     hyper_channels),
                      activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>))
classifier<span style="color:#f92672">.</span>add(MaxPooling2D(pool_size <span style="color:#f92672">=</span> (<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>)))

<span style="color:#75715e"># Flattening</span>
classifier<span style="color:#f92672">.</span>add(Flatten())

<span style="color:#75715e"># Full Connection</span>
classifier<span style="color:#f92672">.</span>add(Dense(units <span style="color:#f92672">=</span> hyper_feature_maps <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;relu&#39;</span>))
classifier<span style="color:#f92672">.</span>add(Dense(units <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>, activation <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;sigmoid&#39;</span>))

<span style="color:#75715e"># Compiling the CNN</span>
classifier<span style="color:#f92672">.</span>compile(optimizer <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;adam&#39;</span>,
                   loss <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;binary_crossentropy&#39;</span>,
                   metrics <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;accuracy&#39;</span>])
</code></pre></div></div>
<h2>5.2. Creating training and validation sets through image flowing with augmentation</h2>
<p><b>Augmentation</b> is an important tool to mitigate the risk of <b>overfitting</b> in machine learning exercises. When it comes to images, augmentation techniques provide modified versions of the original images through a series of specific procedures. As the chosen dataset is not as large as desired for an image processing experience, augmentation can fill this blank and help address the risk of overfitting.</p>
<p>Keras '<b>ImageDataGenerator</b>' object (class) is used in this exercise as the primary tool to generate augmented training and validation image sets. Two instances of this object are created:</p>
<ul>
    <li>'<b>train_gen</b>' will be utilized to generate the training set. As the original images stored in folders (more ahead) correspond to 500x500 pixel matrices of real values ranging from 0 to 255, rescaling is performed through the 'rescale' parameter. Also, augmentation is achieved with some image shearing, zooming and vertical flips.</li>
    <li>'<b>val_gen</b>' will be utilized to generate the validation set. Only rescaling is performed in this case, as we intend to validate the learning process making use of pure images with no manipulation.</li>
</ul>
<p>The '<a href="https://keras.io/api/preprocessing/image/#flowfromdirectory-method" target="_blank">flow_from_directory</a>' method allows for pulling from specific folders images for training and validation, resizing them, grouping them in batches and specifying the machine objective (a binary classification, in this case) and the color mode of choice. The use of 'flow_from_directory' assumes that the data is properly stored in appropriate folders with a pre-determined structure:</p>
<ul>
    <li>train</li>
    <ul>
        <li>normal</li>
        <li>pneumonia</li>
    </ul>
    <li>val</li>
    <ul>
        <li>normal</li>
        <li>pneumonia</li>
    </ul>
</ul>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Creating Training and Validation Image Flows</span>

train_gen <span style="color:#f92672">=</span> ImageDataGenerator(rescale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">255</span>,
                               shear_range <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>,
                               zoom_range <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.2</span>,
                               vertical_flip <span style="color:#f92672">=</span> True)

val_gen <span style="color:#f92672">=</span> ImageDataGenerator(rescale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">255</span>)

train_set <span style="color:#f92672">=</span> train_gen<span style="color:#f92672">.</span>flow_from_directory(<span style="color:#e6db74">&#39;train&#39;</span>,
                                          target_size <span style="color:#f92672">=</span> (hyper_dimension,
                                                         hyper_dimension),
                                          batch_size <span style="color:#f92672">=</span> hyper_batch_size,
                                          class_mode <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;binary&#39;</span>,
                                          color_mode <span style="color:#f92672">=</span> hyper_mode)

val_set <span style="color:#f92672">=</span> val_gen<span style="color:#f92672">.</span>flow_from_directory(<span style="color:#e6db74">&#39;val&#39;</span>,
                                      target_size <span style="color:#f92672">=</span> (hyper_dimension,
                                                     hyper_dimension),
                                      batch_size <span style="color:#f92672">=</span> hyper_batch_size,
                                      class_mode <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;binary&#39;</span>,
                                      color_mode <span style="color:#f92672">=</span> hyper_mode)
</code></pre></div></div>
<p class="codeoutput">Found 4192 images belonging to 2 classes.<br/>
Found 1040 images belonging to 2 classes.</p>
<h2>5.3. Image visualization prior to fitting</h2>
<p>Once training and testing sets are generated, it is important to <b>investigate how images look like</b>.</p>
<p>Understanding the structure of the training and validation sets is important to manipulate them. Let's take the training set as an example.</p>
<ul>
    <li>'<b>train_set [0]</b>' corresponds to the first set of image batches flowed from the source folder;</li>
    <li>'<b>train_set [0] [0]</b>' coresponds to the first batch of 16 images included in the first set of batches;</li>
    <li>Each of the 16 images in this first batch can be accessed via the 'array_to_img' method and properly plotted.</li>
</ul>
<p>The analysis of the first batch in the training set shows that augmentation has been successfully performed. Some images have been slightly shared, zoomed in and/or flipped vertically. Also, it is evident that we are now dealing with grayscale images, as expected.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Visualizing images in the first training set batch </span>

image_batch <span style="color:#f92672">=</span> train_set[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]

plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">20</span>,<span style="color:#ae81ff">5</span>))
<span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(image_batch)):
    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">2</span>,<span style="color:#ae81ff">8</span>,i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>)
    pil_img <span style="color:#f92672">=</span> array_to_img(image_batch[i])
    plt<span style="color:#f92672">.</span>imshow(pil_img,cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;gray&#39;</span>)
    plt<span style="color:#f92672">.</span>axis(<span style="color:#e6db74">&#39;off&#39;</span>)
plt<span style="color:#f92672">.</span>tight_layout()
plt<span style="color:#f92672">.</span>show()
</code></pre></div></div>
<div class='row'>
    <img class="imageboxcentered" src='/detecting-pneumonia/images/pneumonia-first-batch.png' alt='First batch'>
</div>
<h2>5.4. Fitting the model</h2>
<p>As we are dealing with a relatively large dataset and augmentation has been implemented, '<b>fit_generator</b>' is our method of choice for fitting purposes. (<b>Note</b>: 'fit' also supports data augmentation from TensorFlow 2.2.0, so it could also be used in this exercise. As the code may be forked by coders not equipped with TF 2.2.0, the original method, 'fit_generator', has been maintained herein.)</p>
<p>The model will be trained in <b>100 epochs</b>. Comments on this choice will be provided ahead.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Fitting the Model</span>

results <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>fit_generator(train_set,
                                   steps_per_epoch <span style="color:#f92672">=</span> len(train_set),
                                   epochs <span style="color:#f92672">=</span> hyper_epochs,
                                   validation_data <span style="color:#f92672">=</span> val_set,
                                   validation_steps <span style="color:#f92672">=</span> len(val_set))
</code></pre></div></div>
<p class="codeoutput">Epoch 1/100<br/>
262/262 [==============================] - 223s 852ms/step - loss: 0.4320 - accuracy: 0.8030 - val_loss: 0.0853 - val_accuracy: 0.8846<br/>
Epoch 2/100<br/>
262/262 [==============================] - 207s 792ms/step - loss: 0.2575 - accuracy: 0.8941 - val_loss: 0.0297 - val_accuracy: 0.9135<br/>
Epoch 3/100<br/>
262/262 [==============================] - 208s 793ms/step - loss: 0.2130 - accuracy: 0.9077 - val_loss: 0.1284 - val_accuracy: 0.9288<br/>
......<br/>
Epoch 98/100<br/>
262/262 [==============================] - 181s 689ms/step - loss: 0.0365 - accuracy: 0.9883 - val_loss: 8.8554e-05 - val_accuracy:<br/> 0.9712
Epoch 99/100<br/>
262/262 [==============================] - 183s 697ms/step - loss: 0.0410 - accuracy: 0.9847 - val_loss: 0.1523 - val_accuracy: 0.9702<br/>
Epoch 100/100<br/>
262/262 [==============================] - 181s 692ms/step - loss: 0.0245 - accuracy: 0.9890 - val_loss: 0.0027 - val_accuracy: 0.9683</p>
<h2>5.5. Saving the model and its weights</h2>
<p>Fitting a CNN is time and resource consuming. Once the model has been properly fitted, saving it along with its weights is interesting, so that the model can be loaded further without the need of fitting again. The model itself is saved in a .json file, while weights are stored in .h5 format.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Saving the model to JSON for further use</span>

classifier_json <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>to_json()
<span style="color:#66d9ef">with</span> open(<span style="color:#e6db74">&#39;pneumonia_cnn_model.json&#39;</span>, <span style="color:#e6db74">&#39;w&#39;</span>) <span style="color:#66d9ef">as</span> json_file:
    json_file<span style="color:#f92672">.</span>write(classifier_json)

<span style="color:#75715e"># Saving the model weights for further use</span>

classifier<span style="color:#f92672">.</span>save_weights(<span style="color:#e6db74">&#39;pneumonia_cnn_weights.h5&#39;</span>)
</code></pre></div></div>
<h2>5.6. Predicting pneumonia</h2>
<p>The <b>test set</b> must be generated so that predictions on it can be made. The same 'flow_from_directory' method is applied, as images are segregated in different folders for different diagnosis ('normal' and 'pneumonia').</p>
<p>As predictions will be confronted with the actual test image labels for performance assessment, it is assured that <b>no shuffling</b> will be executed. Also, test images will be flown from the corresponding folders individually (batches of <b>1 image each</b>).</p>
<p>The model will provide for each image a scalar output within the range [0,1] corresponding to the probability a given image represents a pneumonia case. In other words, the higher the output, the higher the probability of a pneumonia occurrence.</p>
<p>As we intend to quantify true positive, true negative, false positive and false negative cases and visualize them in a confusion matrix, scalar results will be segregated in two subsets (<b>below and above the 0.5 threshold</b>), corresponding to 'normal' and 'pneumonia' diagnosis, respectively.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Creating Test Image Flow</span>

test_gen <span style="color:#f92672">=</span> ImageDataGenerator(rescale <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span><span style="color:#f92672">/</span><span style="color:#ae81ff">255</span>)

test_set <span style="color:#f92672">=</span> test_gen<span style="color:#f92672">.</span>flow_from_directory(<span style="color:#e6db74">&#39;../input/pneumonia-xray-images/test&#39;</span>,
                                        target_size <span style="color:#f92672">=</span> (hyper_dimension,
                                                       hyper_dimension),
                                        batch_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>,
                                        class_mode <span style="color:#f92672">=</span> None,
                                        color_mode <span style="color:#f92672">=</span> hyper_mode,
                                        shuffle<span style="color:#f92672">=</span>False)

<span style="color:#75715e"># Making Predictions</span>

predictions <span style="color:#f92672">=</span> classifier<span style="color:#f92672">.</span>predict_generator(test_set)
predictions[predictions <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">0.5</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span> predictions[predictions <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span>
</code></pre></div></div>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-06-results" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>6. Results</h1>
<h2>6.1. Classification performance - Confusion matrix</h2>
<p>The segregation described in Section 5.6 allows for the construction of a confusion matrix and calculate the overall prediction accuracy from it.</p>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># Confusion Matrix</span>

cm <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(data<span style="color:#f92672">=</span>confusion_matrix(test_set<span style="color:#f92672">.</span>classes, predictions, labels<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>]),
                  index<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Actual Normal&#34;</span>, <span style="color:#e6db74">&#34;Actual Pneumonia&#34;</span>],
                  columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;Predicted Normal&#34;</span>, <span style="color:#e6db74">&#34;Predicted Pneumonia&#34;</span>])
cm
</code></pre></div></div>
<table>
    <tbody class="codeoutput">
        <tr>
            <td style="width: 28%; border: 0;"></td>
            <td style="width: 36%; border: 0;">Predicted Normal</td>
            <td style="width: 36%; border: 0;">Predicted Opacity</td>
        </tr>
        <tr>
            <td style="width: 28%; border: 0;">Actual Normal</td>
            <td style="width: 36%; border: 0;">195</td>
            <td style="width: 36%; border: 0;">39</td>
        </tr>
        <tr>
            <td style="width: 28%; border: 0;">Actual Stable</td>
            <td style="width: 36%; border: 0;">6</td>
            <td style="width: 36%; border: 0;">384</td>
        </tr>
    </tbody>
</table>

    </div>
  
</div>

    </div>
  </section>

  
  
  

  

  

  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  <section id="section-07-discussion-and-final-remarks" class="home-section wg-blank   " style="padding: 0 0 0 0;" >
    <div class="container">
      


<div class="row">
  
    <div class="col-lg-12">
      
      
      <h1>7. Discussion and final remarks</h1>
<p>Results show that very <b>low losses</b> and <b>accuracies of 96.8%</b> (validation) and <b>92.8%</b> (testing) are obtained with a traditional CNN approach, without any refinement based, for example, on the use of pre-trained image classification models.</p>
<p>In addition, when we deal with medical diagnosis, a false positive (i.e. prediciting illness when the patient is healthy) is <b>less critical</b> than a false negative (predicting healthiness when the patient is sick). The number of false negatives obtained with the CNN presented here is extremely low (<b>only 6 cases</b>, or <b>0.96%</b> of the total number of predictions), which positions the machine developed here as a reliable ancillary tool for pneumonia detection.</p>
<p>Specific considerations on the model and the hyperparameters must be made:</p>
<ol>
    <li>As mentioned above, the approach presented here is very traditional. The CNN architecture adopted here follows those Yann LeCun (recognized as founding father of convolutional networks and their use in computer vision) promoted in the 1990s for image classification. <b>No pre-trained model</b> was used here, and their employment could be taken as a suggestion for further improvements;</li>
    <li>The model was trained several times with a <b>variety of hyperparameter combinations</b>. The final version depicted here was the one through which the best results were obtained. On hyperparameters:</li>
    <ul>
        <li>Tested numbers of epochs were 10, 25, 50 and 100. Reasonable results (low losses and high accuracy on both validation AND testing) were achieved only with <b>50 or 100 epochs</b>. The final training was performed over 100 epochs;</li>
        <li>The model was fed with 400x400, 500x500 and 600x600 pixels images. Best performance was achieved with <b>500x500</b> resized input images;</li>
        <li>Batches of <b>16 images</b> proved to be the best choice over 8, 32 and 64 image alternatives. It must be emphasized though that much faster runtimes could be achieved with the use of TPUs, case in which larger batches would be more appropriate. There is an ongoing discussion on how TPUs handle floating scalars and how large batches should be to extract from TPUs their maximum performance. As this exercise made use of GPUs only, this topic was not explored.</li>
        <li>Finally, the model was trained with both 3-channel (RBG) and 1-channel (grayscale) versions of input images (which are originally 3-channel samples). As a higher number of channels carries in theory a larger amount of information, it was initially expected that 3-channel image trainings would lead to better machines. However, this was not observed - <b>grayscale</b>-based exercises led to hiher accuracies and lower losses. Further investigation could be made, but in a first assessment of such results, considering that in pneumonia detection cases we look for <b>opacities</b> in the lungs, those might be more evidenced in grayscale samples than in colored ones. For now this is just an assumption demanding further analysis.</li>
    </ul>
</ol>
<div class="boxborder">
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">end_time <span style="color:#f92672">=</span> datetime<span style="color:#f92672">.</span>now()
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">Start time&#39;</span>, start_time)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;End time&#39;</span>, end_time)
<span style="color:#66d9ef">print</span>(<span style="color:#e6db74">&#39;Time elapsed&#39;</span>, end_time <span style="color:#f92672">-</span> start_time)
</code></pre></div></div>

    </div>
  
</div>

    </div>
  </section>



      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/instant.page/5.1.0/instantpage.min.js" integrity="sha512-1+qUtKoh9XZW7j+6LhRMAyOrgSQKenQ4mluTR+cvxXjP1Z54RxZuzstR/H9kgPXQsVB8IW7DMDFUJpzLjvhGSQ==" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/highlight.min.js" integrity="sha512-7t8APmYpzEsZP7CYoA7RfMPV9Bb+PJHa9x2WiUnDXZx3XHveuyWUtvNOexhkierl5flZ3tr92dP1mMS+SGlD+A==" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.2/languages/python.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks",
        'slides' : "Slides"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.3b2b658c61ebd725bd5fc606c89fe44c.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  
  <p class="powered-by">
    
      <a href="/privacy/">Privacy Policy</a>
    
    
       &middot; 
      <a href="/terms/">Terms of Service</a>
    
  </p>
  

  <p class="powered-by">
    © 2020 Paulo Breviglieri | <a href="mailto:info@paulobreviglieri.com">info@paulobreviglieri.com</a>
  </p>

</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
